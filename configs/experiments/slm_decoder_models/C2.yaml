model:
  name: "microsoft/phi-4"
  type: "phi4_classification_lora"
  use_essay_prompt: true
  num_labels: 6
  output_dir: "./results/phi4-balanced/C2"
  logging_dir: "./logs/phi4-balanced/C2"
  best_model_dir: "./results/phi4-balanced/C2/best_model"
  lora_r: 8
  lora_dropout: 0.05
  lora_alpha: 16
  lora_target_modules: "all-linear"

dataset:
  grade_index: 1

training_id: "phi4-balanced-C2"

training_params:
  weight_decay: 0.01
  warmup_ratio: 0.1
  learning_rate: 5e-5
  train_batch_size: 1
  eval_batch_size: 16
  gradient_accumulation_steps: 16
  gradient_checkpointing: False