model:
  name: "/tmp/clean_cache/gervasio-8b-portuguese-ptpt-decoder"
  type: "llama31_classification_lora"
  num_labels: 6
  output_dir: "./results/"
  logging_dir: "./logs/"
  best_model_dir: "./results/best_model"
  lora_r: 16
  lora_dropout: 0.1
  lora_alpha: 32
  lora_target_modules: "all-linear"
  # Empty string for new training, or path like "kamel-usp/jbcs2025_phi35-balanced-C1" for loading pre trained
  checkpoint_path: ""

tokenizer:
  name: "/tmp/clean_cache/gervasio-8b-portuguese-ptpt-decoder"

dataset:
  grade_index: 0
  use_full_context: False 

training_params:
  weight_decay: 0.01
  warmup_ratio: 0.1
  learning_rate: 5e-5
  train_batch_size: 8
  eval_batch_size: 4
  gradient_accumulation_steps: 2
  gradient_checkpointing: True