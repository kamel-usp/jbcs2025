{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cedebce",
   "metadata": {},
   "source": [
    "# Tokenization Analysis\n",
    "\n",
    "This notebook analyzes the average sequence length after tokenization for different models and tokenizers.\n",
    "This helps validate which models can receive different tokenization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc38677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = str(Path(\".\").resolve().parent) + \"/scripts\"\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "from preprocess import load_tokenizer, get_tokenize_function #NOQA\n",
    "from models.fine_tuning_models.model_types_enum import ModelTypesEnum #NOQA\n",
    "\n",
    "# Set up logging with a specific handler that we can control\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to clear logs\n",
    "def clear_logs():\n",
    "    \"\"\"Clear all handlers to prevent log accumulation\"\"\"\n",
    "    # Get all loggers\n",
    "    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "    loggers.append(logging.getLogger())  # Add root logger\n",
    "    \n",
    "    # Clear handlers for each logger\n",
    "    for logger_instance in loggers:\n",
    "        handlers = logger_instance.handlers[:]\n",
    "        for handler in handlers:\n",
    "            handler.close()\n",
    "            logger_instance.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137840a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fccd2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "dataset_name = \"kamel-usp/aes_enem_dataset\"\n",
    "dataset_split = \"JBCS2025\"\n",
    "cache_dir = \"/tmp/\"\n",
    "\n",
    "# Model configurations to test\n",
    "model_configs = [\n",
    "    {\n",
    "        \"model_type\": ModelTypesEnum.PHI4_CLASSIFICATION_LORA.value,\n",
    "        \"base_model\": \"microsoft/phi-4\",\n",
    "        \"name\": \"Phi-4\",\n",
    "        \"use_full_context\": False\n",
    "    },\n",
    "    {\n",
    "        \"model_type\": ModelTypesEnum.PHI35_CLASSIFICATION_LORA.value,\n",
    "        \"base_model\": \"microsoft/Phi-3.5-mini-instruct\",\n",
    "        \"name\": \"Phi-3.5\",\n",
    "        \"use_full_context\": False\n",
    "    },\n",
    "    {\n",
    "        \"model_type\": ModelTypesEnum.LLAMA31_CLASSIFICATION_LORA.value,\n",
    "        \"base_model\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"name\": \"Llama-3.1\",\n",
    "        \"use_full_context\": False\n",
    "    },\n",
    "    {\n",
    "        \"model_type\": ModelTypesEnum.ENCODER_CLASSIFICATION.value,\n",
    "        \"base_model\": \"neuralmind/bert-base-portuguese-cased\",\n",
    "        \"name\": \"Bertimbau Base\",\n",
    "        \"use_full_context\": False\n",
    "    },\n",
    "    {\n",
    "        \"model_type\": ModelTypesEnum.ENCODER_CLASSIFICATION.value,\n",
    "        \"base_model\": \"google-bert/bert-base-multilingual-cased\",\n",
    "        \"name\": \"Bert Multilingual\",\n",
    "        \"use_full_context\": False\n",
    "    },\n",
    "    {\n",
    "        \"model_type\": ModelTypesEnum.PHI4_CLASSIFICATION_LORA.value,\n",
    "        \"base_model\": \"microsoft/phi-4\",\n",
    "        \"name\": \"Phi-4-FullContext\",\n",
    "        \"use_full_context\": True\n",
    "    },\n",
    "    {\n",
    "        \"model_type\": ModelTypesEnum.PHI35_CLASSIFICATION_LORA.value,\n",
    "        \"base_model\": \"microsoft/Phi-3.5-mini-instruct\",\n",
    "        \"name\": \"Phi-3.5-FullContext\",\n",
    "        \"use_full_context\": True\n",
    "    },\n",
    "    {\n",
    "        \"model_type\": ModelTypesEnum.LLAMA31_CLASSIFICATION_LORA.value,\n",
    "        \"base_model\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"name\": \"Llama-3.1-FullContext\",\n",
    "        \"use_full_context\": True\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "# Grade indices to test (0-4 for C1-C5)\n",
    "grade_indices = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78debc69",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c187b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 500 training samples\n",
      "Dataset splits: ['train', 'validation', 'test']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\n",
    "    dataset_name,\n",
    "    dataset_split,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset['train'])} training samples\")\n",
    "print(f\"Dataset splits: {list(dataset.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07df8a",
   "metadata": {},
   "source": [
    "## Tokenization Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a9c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tokenization(dataset, tokenizer, model_type, grade_index, text_column=\"essay_text\", logger=None, use_full_context=False):\n",
    "    \"\"\"\n",
    "    Analyze tokenization for a specific model and grade index.\n",
    "    Returns statistics about sequence lengths.\n",
    "    \"\"\"\n",
    "    # Get the tokenization function\n",
    "    tokenize_function = get_tokenize_function(\n",
    "        model_type=model_type,\n",
    "        tokenizer=tokenizer,\n",
    "        text_column=text_column,\n",
    "        grade_index=grade_index,\n",
    "        logger=logger,\n",
    "        use_full_context=use_full_context\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Apply tokenization\n",
    "    tokenized_sample = dataset['train'].map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Extract sequence lengths\n",
    "    sequence_lengths = []\n",
    "    for example in tokenized_sample:\n",
    "        if 'input_ids' in example:\n",
    "            sequence_lengths.append(len(example['input_ids']))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'mean': np.mean(sequence_lengths),\n",
    "        'std': np.std(sequence_lengths),\n",
    "        'min': np.min(sequence_lengths),\n",
    "        'max': np.max(sequence_lengths),\n",
    "        'median': np.median(sequence_lengths),\n",
    "        'p95': np.percentile(sequence_lengths, 95),\n",
    "        'p99': np.percentile(sequence_lengths, 99)\n",
    "    }\n",
    "    \n",
    "    return stats, sequence_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0386dd4",
   "metadata": {},
   "source": [
    "## Run Tokenization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0faa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Phi-4 (microsoft/phi-4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:06,958 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:07,797 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:08,910 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148a2b31268c40f18e5e02fca952426b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:10,532 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:11,462 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C5...\n",
      "\n",
      "Analyzing Phi-3.5 (microsoft/Phi-3.5-mini-instruct)...\n",
      "\n",
      "Analyzing Phi-3.5 (microsoft/Phi-3.5-mini-instruct)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:12,744 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:13,646 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:14,916 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3907efd25ce241169412292eb3cd9262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:16,623 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:17,655 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C5...\n",
      "\n",
      "Analyzing Llama-3.1 (meta-llama/Llama-3.1-8B)...\n",
      "\n",
      "Analyzing Llama-3.1 (meta-llama/Llama-3.1-8B)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:19,239 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:20,125 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fdf75ef4d7421c92a437ea29a14797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:22,180 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:23,155 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:24,124 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C5...\n",
      "\n",
      "Analyzing Bertimbau Base (neuralmind/bert-base-portuguese-cased)...\n",
      "\n",
      "Analyzing Bertimbau Base (neuralmind/bert-base-portuguese-cased)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:25,425 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:25,708 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:25,992 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:26,276 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:26,561 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C5...\n",
      "\n",
      "Analyzing Bert Multilingual (google-bert/bert-base-multilingual-cased)...\n",
      "\n",
      "Analyzing Bert Multilingual (google-bert/bert-base-multilingual-cased)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:27,230 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:27,529 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:27,825 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:28,131 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:28,432 - __main__ - INFO - Tokenizer function parameters- Padding:max_length; Truncation: True; Use Full Context: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C5...\n",
      "\n",
      "Analyzing Phi-4-FullContext (microsoft/phi-4)...\n",
      "\n",
      "Analyzing Phi-4-FullContext (microsoft/phi-4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:29,090 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:30,251 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb9b97474614d7c8fc08f286b3ef672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:32,930 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:34,176 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:35,423 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C5...\n",
      "\n",
      "Analyzing Phi-3.5-FullContext (microsoft/Phi-3.5-mini-instruct)...\n",
      "\n",
      "Analyzing Phi-3.5-FullContext (microsoft/Phi-3.5-mini-instruct)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:37,046 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:38,348 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ffd0e7ad78480cbb50f2327d5aec7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:41,171 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:42,607 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:44,029 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C5...\n",
      "\n",
      "Analyzing Llama-3.1-FullContext (meta-llama/Llama-3.1-8B)...\n",
      "\n",
      "Analyzing Llama-3.1-FullContext (meta-llama/Llama-3.1-8B)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:46,005 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:47,230 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67de4ae6f7a4de392c2cb9598617021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:50,001 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:51,317 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:55:52,643 - __main__ - INFO - Tokenizer function parameters- Padding:longest; Truncation: False; Use Full Context: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing grade C5...\n"
     ]
    }
   ],
   "source": [
    "# Store results\n",
    "results = []\n",
    "all_sequence_lengths = {}\n",
    "\n",
    "for config in model_configs:\n",
    "    model_type = config['model_type']\n",
    "    base_model = config['base_model']\n",
    "    model_name = config['name']\n",
    "    use_full_context = config.get('use_full_context', False)\n",
    "    \n",
    "    print(f\"\\nAnalyzing {model_name} ({base_model})...\")\n",
    "    \n",
    "    try:\n",
    "    # Load tokenizer\n",
    "        tokenizer = load_tokenizer(model_type, base_model, cache_dir)\n",
    "\n",
    "        for grade_index in grade_indices:\n",
    "            print(f\"  Processing grade C{grade_index + 1}...\")\n",
    "\n",
    "            # Clear logs before processing\n",
    "            clear_logs()\n",
    "\n",
    "            # Re-setup logger after clearing\n",
    "            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            logger = logging.getLogger(__name__)\n",
    "\n",
    "            # Analyze tokenization\n",
    "            stats, seq_lengths = analyze_tokenization(\n",
    "                dataset=dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                model_type=model_type,\n",
    "                grade_index=grade_index,\n",
    "                text_column=\"essay_text\",\n",
    "                logger=logger,\n",
    "                use_full_context=use_full_context\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            result = {\n",
    "                'model': model_name,\n",
    "                'model_type': model_type,\n",
    "                'grade': f'C{grade_index + 1}',\n",
    "                **stats\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Store sequence lengths for visualization\n",
    "            key = f\"{model_name}_C{grade_index + 1}\"\n",
    "            all_sequence_lengths[key] = seq_lengths\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {model_name}: {str(e)}\")\n",
    "        continue\n",
    "    # Clear logs after each model\n",
    "    clear_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3eb270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_type</th>\n",
       "      <th>grade</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "      <th>p95</th>\n",
       "      <th>p99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Phi-3.5</td>\n",
       "      <td>phi35_classification_lora</td>\n",
       "      <td>C3</td>\n",
       "      <td>3193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3193</td>\n",
       "      <td>3193</td>\n",
       "      <td>3193.0</td>\n",
       "      <td>3193.0</td>\n",
       "      <td>3193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Phi-3.5</td>\n",
       "      <td>phi35_classification_lora</td>\n",
       "      <td>C1</td>\n",
       "      <td>2775.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2775</td>\n",
       "      <td>2775</td>\n",
       "      <td>2775.0</td>\n",
       "      <td>2775.0</td>\n",
       "      <td>2775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phi-4</td>\n",
       "      <td>phi4_classification_lora</td>\n",
       "      <td>C4</td>\n",
       "      <td>2780.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2780</td>\n",
       "      <td>2780</td>\n",
       "      <td>2780.0</td>\n",
       "      <td>2780.0</td>\n",
       "      <td>2780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Phi-3.5-FullContext</td>\n",
       "      <td>phi35_classification_lora</td>\n",
       "      <td>C4</td>\n",
       "      <td>4474.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4474</td>\n",
       "      <td>4474</td>\n",
       "      <td>4474.0</td>\n",
       "      <td>4474.0</td>\n",
       "      <td>4474.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Phi-3.5-FullContext</td>\n",
       "      <td>phi35_classification_lora</td>\n",
       "      <td>C2</td>\n",
       "      <td>5335.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5335</td>\n",
       "      <td>5335</td>\n",
       "      <td>5335.0</td>\n",
       "      <td>5335.0</td>\n",
       "      <td>5335.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model                 model_type grade    mean  std   min  \\\n",
       "7               Phi-3.5  phi35_classification_lora    C3  3193.0  0.0  3193   \n",
       "5               Phi-3.5  phi35_classification_lora    C1  2775.0  0.0  2775   \n",
       "3                 Phi-4   phi4_classification_lora    C4  2780.0  0.0  2780   \n",
       "33  Phi-3.5-FullContext  phi35_classification_lora    C4  4474.0  0.0  4474   \n",
       "31  Phi-3.5-FullContext  phi35_classification_lora    C2  5335.0  0.0  5335   \n",
       "\n",
       "     max  median     p95     p99  \n",
       "7   3193  3193.0  3193.0  3193.0  \n",
       "5   2775  2775.0  2775.0  2775.0  \n",
       "3   2780  2780.0  2780.0  2780.0  \n",
       "33  4474  4474.0  4474.0  4474.0  \n",
       "31  5335  5335.0  5335.0  5335.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame with results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b02d0c",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd527b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sequence Length by Model and Grade:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>grade</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bert Multilingual</th>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bertimbau Base</th>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.1</th>\n",
       "      <td>2479.0</td>\n",
       "      <td>3470.0</td>\n",
       "      <td>2774.0</td>\n",
       "      <td>2768.0</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.1-FullContext</th>\n",
       "      <td>3600.0</td>\n",
       "      <td>4591.0</td>\n",
       "      <td>3895.0</td>\n",
       "      <td>3889.0</td>\n",
       "      <td>4042.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3.5</th>\n",
       "      <td>2775.0</td>\n",
       "      <td>4042.0</td>\n",
       "      <td>3193.0</td>\n",
       "      <td>3181.0</td>\n",
       "      <td>3349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3.5-FullContext</th>\n",
       "      <td>4068.0</td>\n",
       "      <td>5335.0</td>\n",
       "      <td>4486.0</td>\n",
       "      <td>4474.0</td>\n",
       "      <td>4642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-4</th>\n",
       "      <td>2489.0</td>\n",
       "      <td>3481.0</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>2780.0</td>\n",
       "      <td>2932.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-4-FullContext</th>\n",
       "      <td>3606.0</td>\n",
       "      <td>4598.0</td>\n",
       "      <td>3902.0</td>\n",
       "      <td>3897.0</td>\n",
       "      <td>4049.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "grade                      C1      C2      C3      C4      C5\n",
       "model                                                        \n",
       "Bert Multilingual       512.0   512.0   512.0   512.0   512.0\n",
       "Bertimbau Base          512.0   512.0   512.0   512.0   512.0\n",
       "Llama-3.1              2479.0  3470.0  2774.0  2768.0  2921.0\n",
       "Llama-3.1-FullContext  3600.0  4591.0  3895.0  3889.0  4042.0\n",
       "Phi-3.5                2775.0  4042.0  3193.0  3181.0  3349.0\n",
       "Phi-3.5-FullContext    4068.0  5335.0  4486.0  4474.0  4642.0\n",
       "Phi-4                  2489.0  3481.0  2785.0  2780.0  2932.0\n",
       "Phi-4-FullContext      3606.0  4598.0  3902.0  3897.0  4049.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "print(\"Average Sequence Length by Model and Grade:\")\n",
    "pivot_table = results_df.pivot_table(\n",
    "    values='mean', \n",
    "    index='model', \n",
    "    columns='grade', \n",
    "    aggfunc='first'\n",
    ")\n",
    "pivot_table.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
