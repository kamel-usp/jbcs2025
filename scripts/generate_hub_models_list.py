"""
Generate a list of Hugging Face Hub model IDs for batch inference.
This script creates all combinations of models, concepts, contexts, and LoRA ranks.
"""

import argparse
from pathlib import Path
from typing import List


def generate_model_list() -> List[str]:
    """
    Generate all model ID combinations based on the specified pattern.
    
    Returns:
        List of model IDs
    """
    # Define the base model prefixes for LoRA models
    lora_model_prefixes = [
        "kamel-usp/jbcs2025_Llama-3.1-8B-llama31_classification_lora",
        "kamel-usp/jbcs2025_phi-4-phi4_classification_lora",
        "kamel-usp/jbcs2025_Phi-3.5-mini-instruct-phi35_classification_lora"
    ]
    
    # Define the encoder model prefixes
    encoder_model_prefixes = [
        "kamel-usp/jbcs2025_bert-base-portuguese-cased-encoder_classification",
        "kamel-usp/jbcs2025_bert-base-multilingual-cased-encoder_classification",
        "kamel-usp/jbcs2025_BERTugues-base-portuguese-cased-encoder_classification",
        "kamel-usp/jbcs2025_bert-large-portuguese-cased-encoder_classification"
    ]
    
    # Define the concepts
    concepts = ["C1", "C2", "C3", "C4", "C5"]
    
    # Define the contexts
    contexts = ["essay_only", "full_context"]
    
    # Define the LoRA ranks
    lora_ranks = ["r8", "r16"]
    
    # Generate all combinations
    model_ids = []
    
    # Generate LoRA model combinations
    for model_prefix in lora_model_prefixes:
        for concept in concepts:
            for context in contexts:
                for lora_rank in lora_ranks:
                    # Build the model ID
                    model_id = f"{model_prefix}-{concept}-{context}-{lora_rank}"
                    model_ids.append(model_id)
    
    # Generate encoder model combinations (no LoRA ranks)
    for model_prefix in encoder_model_prefixes:
        for concept in concepts:
            for context in contexts:
                if context == "full_context":
                    continue # Skip full context for encoder models
                # Build the model ID
                model_id = f"{model_prefix}-{concept}-{context}"
                model_ids.append(model_id)
    
    return model_ids


def write_models_file(output_path: Path, model_ids: List[str]) -> None:
    """
    Write the model IDs to a file.
    
    Args:
        output_path: Path to the output file
        model_ids: List of model IDs to write
    """
    with open(output_path, 'w') as f:
        # Write header
        f.write("# List of Hugging Face Hub models to run inference on\n")
        f.write("# One model ID per line\n")
        f.write("# Lines starting with # are ignored\n")
        f.write("# Auto-generated by generate_hub_models_list.py\n\n")
        
        # Group by model type for better organization
        current_model_type = None
        
        for model_id in model_ids:
            # Extract model type from the ID
            if "Llama-3.1-8B" in model_id:
                model_type = "Llama 3.1 8B"
            elif "phi-4" in model_id:
                model_type = "Phi-4"
            elif "Phi-3.5-mini-instruct" in model_id:
                model_type = "Phi-3.5"
            elif "bert-base-portuguese-cased-encoder" in model_id:
                model_type = "BERT Base Portuguese"
            elif "bert-base-multilingual-cased-encoder" in model_id:
                model_type = "BERT Base Multilingual"
            elif "BERTugues-base-portuguese-cased-encoder" in model_id:
                model_type = "BERTugues Base Portuguese"
            elif "bert-large-portuguese-cased-encoder" in model_id:
                model_type = "BERT Large Portuguese"
            else:
                model_type = "Unknown"
            
            # Add section header if model type changed
            if model_type != current_model_type:
                if current_model_type is not None:
                    f.write("\n")
                f.write(f"# {model_type} models\n")
                current_model_type = model_type
            
            # Write the model ID
            f.write(f"{model_id}\n")
    
    print(f"Generated {len(model_ids)} model IDs and saved to {output_path}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Generate a list of HF Hub model IDs for batch inference"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="configs/hub_models_list.txt",
        help="Output file path (default: configs/hub_models_list.txt)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Print the model IDs without writing to file",
    )
    
    args = parser.parse_args()
    
    # Generate model IDs
    model_ids = generate_model_list()
    
    if args.dry_run:
        print(f"Would generate {len(model_ids)} model IDs:")
        for model_id in model_ids:
            print(f"  {model_id}")
    else:
        # Create output directory if it doesn't exist
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Write to file
        write_models_file(output_path, model_ids)
        
        # Print some statistics
        print(f"\nStatistics:")
        print(f"  Total models: {len(model_ids)}")
        print(f"  LoRA models: {3 * 5 * 2 * 2}")  # 3 models * 5 concepts * 2 contexts * 2 ranks
        print(f"  Encoder models: {4 * 5 * 2}")   # 4 models * 5 concepts * 2 contexts
        print(f"  Combinations per LoRA model: {5 * 2 * 2}")
        print(f"  Combinations per encoder model: {5 * 2}")


if __name__ == "__main__":
    main()
